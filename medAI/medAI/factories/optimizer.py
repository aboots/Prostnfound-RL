import argparse
from dataclasses import dataclass
from typing import Literal, Mapping, Optional
import torch
from torch.optim.lr_scheduler import LambdaLR
from medAI.utils.lr_scheduler import LinearWarmupCosineAnnealing
from medAI.utils.distributed import get_world_size
from medAI.utils.cosine_scheduler import cosine_scheduler
from medAI.optimizer.lars import LARS
import logging


__all__ = [
    "build_optimizer_v0",
    "build_optimizer_v1",
    "OptimizerCfg",
    "OptimGroupCfg",
]


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def build_optimizer_v0(
    model,
    niter_per_ep,
    warmup_epochs=0,
    lr=1e-4,
    wd=0,
    batch_size_per_gpu=32,
    optimizer_type: Literal["adamw", "sgd"] = "adamw",
    scheduler_type: Literal["cosine", "constant"] = "cosine",
    scheduler_epochs=100,
):
    """
    Build a simple optimizer and scheduler without parameter groups.
    """

    lr = lr * batch_size_per_gpu / 256

    if optimizer_type == "adamw":
        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)
    elif optimizer_type == "sgd":
        optimizer = torch.optim.SGD(
            model.parameters(), lr=lr, momentum=0.9, weight_decay=wd
        )
    else:
        raise ValueError(f"Unsupported optimizer type: {optimizer_type}")

    if scheduler_type == "cosine":
        scheduler = LambdaLR(
            optimizer,
            LinearWarmupCosineAnnealing(
                0, warmup_epochs, scheduler_epochs, niter_per_ep
            ),
        )
    else:
        scheduler = LambdaLR(optimizer, lambda it: 1)

    return optimizer, scheduler


@dataclass
class OptimGroupCfg:
    base_lr: float
    min_lr: float
    warmup_epochs: int = 0
    frozen_epochs: int = 0
    weight_decay: float = 0.04
    weight_decay_end: float = 0.4
    num_annealing_phases: int = 1


class OptimizerCfg:

    type: Literal["adamw", "sgd", "lars"] = "adamw"
    momentum_teacher: float = 0.996
    clip_grad: Optional[float] = None
    param_groups_config: dict[str, OptimGroupCfg] = None

    def __init__(
        self,
        *,
        type: Literal["adamw", "sgd", "lars"],
        momentum_teacher: float,
        clip_grad: Optional[float],
        **param_groups_configs: OptimGroupCfg | dict,
    ):
        self.type = type
        self.momentum_teacher = momentum_teacher
        self.clip_grad = clip_grad
        
        self.param_groups_config = {}
        for group_name, cfg in param_groups_configs.items():
            if isinstance(cfg, Mapping):
                cfg = OptimGroupCfg(**cfg)
            self.param_groups_config[group_name] = cfg


def build_optimizer_v1(
    model_dict, num_iters_per_epoch, conf: OptimizerCfg, batch_size_per_gpu, epochs
):
    """
    Build optimizer with parameter groups, each having its own learning rate and weight decay schedules.
    Also builds a momentum schedule, lr schedule, and weight decay schedule.
    """

    def compute_true_lr_from_base_lr(base_lr):
        return base_lr * batch_size_per_gpu * get_world_size() / 256.0

    params_groups = []
    lr_schedulers = []
    wd_schedulers = []

    named_parameter_groups = {k: v.named_parameters() for k, v in model_dict.items()}

    for group_name, named_parameters in named_parameter_groups.items():

        named_parameters = list(named_parameters)
        logging.info(
            f"Creating optimizer group for {group_name}: {len(named_parameters)} params, config: {conf.param_groups_config.get(group_name)}"
        )

        opt_conf_for_group = conf.param_groups_config.get(group_name)
        if opt_conf_for_group is None:
            raise ValueError(
                f"Trying to configure optimizer for group {group_name}, but did not find it in config (keys {list(conf.param_groups_config.keys())})"
            )

        # get regularized vs. non-regularized parameter groups
        regularized = []
        not_regularized = []
        for name, param in named_parameters:
            if not param.requires_grad:
                continue
            # we do not regularize biases nor Norm parameters
            if name.endswith(".bias") or len(param.shape) == 1:
                not_regularized.append(param)
            else:
                regularized.append(param)

        regularized = {"params": regularized}
        params_groups.append(regularized)
        not_regularized = {"params": not_regularized, "weight_decay": 0.0}
        params_groups.append(not_regularized)
        for _ in range(
            2
        ):  # lr has the same schedule for regularized vs. non_regularized
            lr_schedulers.append(
                cosine_scheduler(
                    compute_true_lr_from_base_lr(
                        opt_conf_for_group.base_lr
                    ),  # linear scaling rule
                    opt_conf_for_group.min_lr,
                    epochs,
                    num_iters_per_epoch,
                    warmup_epochs=opt_conf_for_group.warmup_epochs,
                    frozen_epochs=opt_conf_for_group.frozen_epochs,
                    num_annealing_phases=opt_conf_for_group.num_annealing_phases,
                )
            )
        wd_schedulers.append(
            cosine_scheduler(
                opt_conf_for_group.weight_decay,
                opt_conf_for_group.weight_decay_end,
                epochs,
                num_iters_per_epoch,
            )
        )
        # second group receives 0 wd
        wd_schedulers.append(cosine_scheduler(0, 0, epochs, num_iters_per_epoch))

    # ============ preparing optimizer ... ============
    if conf.type == "adamw":
        optimizer = torch.optim.AdamW(params_groups)  # to use with ViTs
    elif conf.type == "sgd":
        optimizer = torch.optim.SGD(
            params_groups, lr=0, momentum=0.9
        )  # lr is set by scheduler
    elif conf.type == "lars":
        optimizer = LARS(params_groups)  # to use with convnet and large batches
    else:
        raise ValueError(f"Unknown optimizer type: {conf.type}")

    # momentum parameter is increased to 1. during training with a cosine schedule
    momentum_schedule = cosine_scheduler(
        conf.momentum_teacher, 1, epochs, num_iters_per_epoch
    )

    return optimizer, lr_schedulers, wd_schedulers, momentum_schedule
