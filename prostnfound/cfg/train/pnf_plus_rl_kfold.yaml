# ProstNFound-RL training configuration
# This config adds RL-guided attention to ProstNFound+

data:
  fold: 0
  n_folds: 5
  test_center: null
  exclude_benign_cores_from_positive_patients: true
  involvement_threshold_pct: null
  undersample_benign_ratio: null
  cohort_selection_mode: kfold
  dataset: default
  batch_size: 8
  num_workers: 8
  train_subsample_seed: 42
  augmentations: translate
  image_size: 256
  first_downsample_size: null
  mask_size: 64
  limit_train_data: null
  mean: [0, 0, 0]
  std: [1, 1, 1]
  flip_ud: false
  frames: first
  crop_to_prostate: false
  rf_as_bmode: false
  include_rf: false

# Model configuration
model: prostnfound_rl_adapter_medsam_legacy
model_kw:
  prompts: [psa,age]
  use_class_decoder: true
  # RL-specific parameters
  num_attention_points: 3  # Number of attention points to identify
  policy_type: categorical  # 'categorical' or 'gaussian'
  policy_hidden_dim: 512
  use_clinical_in_policy: true
  freeze_prostnfound: false  # Set to true to only train RL policy
  temperature: 1.0

# Loss configuration
loss: mil_prop_bce_entropy_reg
outside_prostate_penalty: false
add_image_clf: true
image_clf_mode: cspca

# RL-specific training parameters
use_rl: true  # Enable RL training mode
rl_mode: grpo  # RL algorithm (grpo, ppo, reinforce)
rl_reward_mode: loss_based  # 'loss_based', 'accuracy_based', or 'combined'
rl_cspca_bonus: 2.0  # Bonus multiplier for csPCa cases
rl_normalize_rewards: true  # Only used when num_samples_per_image=1
rl_num_update_epochs: 4  # Number of GRPO update epochs per batch
rl_clip_eps: 0.2  # PPO clipping epsilon
rl_entropy_coef: 0.01  # Entropy coefficient
rl_value_coef: 0.5  # Value loss coefficient
rl_max_grad_norm: 0.5  # Gradient clipping
rl_gamma: 1.0  # Discount factor (1.0 = no discounting)

# Within-Image Comparison (key improvement)
# Instead of comparing performance across different images (which have different difficulty),
# we sample multiple attention location configurations per image and compare within each image.
# This allows fair comparison and helps the model learn which attention strategies work best.
rl_num_samples_per_image: 4  # Number of rollouts per image (set to 1 to disable)

name: "prostnfound-rl-within-image-v2"  # Changed name for fresh run

# Optimizer configuration
optimizer: adamw
lr: 1.0e-05
encoder_lr: 1.0e-05  # Lower LR for encoder (or 0 if frozen)
warmup_lr: 0.0001
cnn_lr: 1.0e-05
warmup_epochs: 0
wd: 0
epochs: 20
accumulate_grad_steps: 1
cutoff_epoch: null

run_test: false
test_every_epoch: false
run_val: true

project: prostnfound_rl
seed: 30
tracked_metric: val/core_auc_high_involvement

wandb:
  mode: online  # Set to "online" to sync to wandb.ai (requires authentication)
  # mode: offline  # Set to "offline" to save locally only

save_checkpoint_wandb: false
torch_compile: false  # Disable for RL (can cause issues with dynamic graphs)
use_amp: true
device: cuda
checkpoint_dir: checkpoints_rl_variations/${name} # New checkpoint directory for fresh run
model_checkpoint: null  # No checkpoint - train from scratch
debug: false
save_best_weights: true

evaluator:
  log_images: false
  log_images_every: 100

